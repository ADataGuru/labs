# Batch Unix:


> Le systÃ¨me Unix permet dâ€™analyser un fichier simplement enchaÃ®nant un ensemble de commandes. 
> Cela est rendu possible grÃ¢ce Ã  une interface commune qui permet de transfÃ©rer simplement de la donnÃ©e
> dâ€™une Ã©tape Ã  lâ€™autre !


Prenons comme exemple un serveur web qui ajoute une ligne Ã  fichier log Ã  chaque fois qu'il sert une requÃªte. Par exemple, en utilisant le format de log par dÃ©faut de nginx, une ligne de log devrait ressembler Ã  : 

```
216.58.210.78 - - [05/May/2022:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
200 3377 "https://www.dataguru.fr" "Mozilla/5.0 (Macintosh; Intel Mac OS X
10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115
Safari/537.36"
```

Si on essaie de retrouver la dÃ©finition de ce log, Ã§a ressemblerait Ã  : 

```
$remote_addr - $remote_user [$time_local] "$request"
$status $body_bytes_sent "$http_referer" "$http_user_agent"
```

Cette ligne de log indique que le 27 fÃ©vrier 2015, Ã  17:55:11 UTC, le serveur a reÃ§u une requÃªte pour le
fichier /css/typography.css de l'adresse IP du client 216.58.210.78. L'utilisateur n'Ã©tait pas authentifiÃ©, donc 
$remote_user est dÃ©fini comme un tiret(-). Le statut de la rÃ©ponse est 200 (c'est-Ã -dire que la demande a abouti), 
et la rÃ©ponse Ã©tait de 3 377 octets. Le navigateur Web Ã©tait Chrome 40, et il a chargÃ© la page https://www.dataguru.fr


Analiser un fichier de log:
---------------------------

Par exemple, disons que vous voulez trouver les cinq pages les plus populaires de votre site Web. Vous pouvez
faire cela dans un shell Unix de la maniÃ¨re suivante:

```shell
cat /var/log/nginx/access.log | # 1ï¸âƒ£
 awk '{print $7}' |             # 2ï¸âƒ£
 sort |                         # 3ï¸âƒ£
 uniq -c |                      # 4ï¸âƒ£
 sort -r -n |                   # 5ï¸âƒ£
 head -n 5                      # 6ï¸âƒ£
```

+ 1ï¸âƒ£ Lire le fichier log
+ 2ï¸âƒ£ Divise chaque ligne en champs par des espaces, et ne garde que le septiÃ¨me de ces champs
de chaque ligne, qui se trouve Ãªtre l'URL demandÃ©e. Dans notre exemple, cette URL  est /css/typography.css.
+ 3ï¸âƒ£ Trier par ordre alphabÃ©tique la liste des URLs demandÃ©es.
+ 4ï¸âƒ£ La commande uniq filtre les lignes rÃ©pÃ©tÃ©es dans son entrÃ©e en vÃ©rifiant si deux lignes adjacentes sont identiques. L'option -c lui demande d'afficher un compteur : pour chaque chaque URL distincte, elle indique combien de fois cette URL est apparue dans l'entrÃ©e.
+ 5ï¸âƒ£ Le deuxiÃ¨me tri est effectuÃ© en fonction du nombre (-n) au dÃ©but de chaque ligne, qui correspond au nombre de fois que l'URL a Ã©tÃ© requÃªtÃ©e. Il renvoie ensuite les rÃ©sultats dans l'ordre inverse (-r), c'est-Ã -dire en commenÃ§ant par le nombre le plus Ã©levÃ©.
+ 6ï¸âƒ£ Enfin, head ne sort que les cinq premiÃ¨res lignes (-n 5) de l'entrÃ©e, et rejette le reste.

> ðŸ’¡ Bien que la cli parait souvent obscure et non familier, nÃ©anmoins Ã§a reste un outil puissant. 
> On peut traiter des Go de logs en quelques secondes. En plus d'Ãªtre performant, Ã§a reste trÃ¨s custom, au lieu de retrouver
> les pages les plus visitÃ©es on pourrait afficher le top 10 des ips des clients les plus importants en changeant 
> l'argument d'awk par '{print $1}'